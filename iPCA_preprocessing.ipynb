{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "In this case dataset file is too large for our RAM and we can not read it as usual.\nEvery entry has 13 features and one binary label. In order to avoid \"Out-of-memory\" errors, we need to preprocess our dataset.\nFor this purpose i use StandardScaler for scaling and IncrementalPCA for preprocessing data."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import IncrementalPCA\nfrom sklearn.preprocessing import StandardScaler",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "I select 10000 rows per chunk to be sure, that it will be finished (i have only 2 Gb RAM)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "chunk_size = 10000\ncomponents = 3",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Only three components is not good idea, but i have huge dataset and after preprocessing my data also can be large."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "for chunk in pd.read_csv ('train.tar.gz', compression = 'gzip', sep = ';', header = 0, quotechar = '\"', chunksize = chunk_size):\n    labels = chunk.iloc [:, 2]\n    selected_features = chunk.iloc [:, [3:16]]\n    \n    scaled_features = StandardScaler (). fit_transform (selected_features)\n    \n    ipca = IncrementalPCA (n_components = components)\n    principalComponents = ipca.fit_transform (scaled_features)\n    preprocessed_data = pd.DataFrame (data = principalComponents)\n    \n    merged_data = pd.concat ([preprocessed_data, labels], axis = 1)\n    \n    merged_data.to_csv ('preprocessed_data.csv', mode = 'w', sep = ';', header = 0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "P.S. Of course chunksize and number of components depends on your available computer resources."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}